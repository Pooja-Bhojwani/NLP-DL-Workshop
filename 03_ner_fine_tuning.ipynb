{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ner_fine_tuning.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"tfgk7q3BZb1F","executionInfo":{"status":"ok","timestamp":1605498436585,"user_tz":300,"elapsed":2896,"user":{"displayName":"pooja bhojwani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIF5HRqafp6-a3HMG54CxPkG_y5D_iDJvM287H9w=s64","userId":"06734995681986157428"}},"outputId":"7f7891f1-d3e9-47e0-f00d-768e1943cb30","colab":{"base_uri":"https://localhost:8080/"}},"source":["! pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.1)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n","Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ng0hpTx-Z6jY"},"source":["# This notebook for NER fine tuning is one of hugging face examples converted to colab notebook\n","\n","# Ref : https://huggingface.co/transformers/custom_datasets.html"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RgQmQAXVZkjj","executionInfo":{"status":"ok","timestamp":1605498438775,"user_tz":300,"elapsed":853,"user":{"displayName":"pooja bhojwani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIF5HRqafp6-a3HMG54CxPkG_y5D_iDJvM287H9w=s64","userId":"06734995681986157428"}},"outputId":"2cac7d8b-0a1c-49d6-9d51-252d64c354b3","colab":{"base_uri":"https://localhost:8080/"}},"source":["!wget http://noisy-text.github.io/2017/files/wnut17train.conll # Lets get the data"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-11-16 03:47:18--  http://noisy-text.github.io/2017/files/wnut17train.conll\n","Resolving noisy-text.github.io (noisy-text.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n","Connecting to noisy-text.github.io (noisy-text.github.io)|185.199.108.153|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 493781 (482K) [application/octet-stream]\n","Saving to: ‘wnut17train.conll.1’\n","\n","\rwnut17train.conll.1   0%[                    ]       0  --.-KB/s               \rwnut17train.conll.1 100%[===================>] 482.21K  --.-KB/s    in 0.03s   \n","\n","2020-11-16 03:47:18 (17.3 MB/s) - ‘wnut17train.conll.1’ saved [493781/493781]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"q6Hwv2smZzKo"},"source":["In this case, we’ll just download the train set, which is a single text file. Each line of the file contains either (1) a word and tag separated by a tab, or (2) a blank line indicating the end of a document. Let’s write a function to read this in. We’ll take in the file path and return token_docs which is a list of lists of token strings, and token_tags which is a list of lists of tag strings."]},{"cell_type":"code","metadata":{"id":"TiefDWalZtGz"},"source":["from pathlib import Path\n","import re\n","\n","def read_wnut(file_path):\n","    file_path = Path(file_path)\n","\n","    raw_text = file_path.read_text().strip()\n","    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n","    token_docs = []\n","    tag_docs = []\n","    for doc in raw_docs:\n","        tokens = []\n","        tags = []\n","        for line in doc.split('\\n'):\n","            token, tag = line.split('\\t')\n","            tokens.append(token)\n","            tags.append(tag)\n","        token_docs.append(tokens)\n","        tag_docs.append(tags)\n","\n","    return token_docs, tag_docs\n","\n","texts, tags = read_wnut('wnut17train.conll')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dSJ6CtFYaH-s","executionInfo":{"status":"ok","timestamp":1605498447096,"user_tz":300,"elapsed":1284,"user":{"displayName":"pooja bhojwani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIF5HRqafp6-a3HMG54CxPkG_y5D_iDJvM287H9w=s64","userId":"06734995681986157428"}},"outputId":"f1f982b4-0e4e-433f-8e2c-3c81e35f0e40","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(texts[0][10:17], tags[0][10:17], sep='\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['for', 'two', 'weeks', '.', 'Empire', 'State', 'Building']\n","['O', 'O', 'O', 'O', 'B-location', 'I-location', 'I-location']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DDUTn91MaPR6"},"source":["from sklearn.model_selection import train_test_split\n","train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2) # split into train and validation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M5X-fTNUa2Ou"},"source":["# The tags we created above needs to be converted into Ids\n","\n","unique_tags = set(tag for doc in tags for tag in doc)\n","tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n","id2tag = {id: tag for tag, id in tag2id.items()} # just inverse mapping"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8NWt75QdbADJ"},"source":["To encode the tokens, we’ll use a pre-trained DistilBert tokenizer. We can tell the tokenizer that we’re dealing with ready-split tokens rather than full sentence strings by passing is_split_into_words=True. We’ll also pass padding=True and truncation=True to pad the sequences to be the same length. Lastly, we can tell the model to return information about the tokens which are split by the wordpiece tokenization process, which we will need in a moment."]},{"cell_type":"code","metadata":{"id":"VClO0bIHKkOm","executionInfo":{"status":"ok","timestamp":1605498460607,"user_tz":300,"elapsed":1041,"user":{"displayName":"pooja bhojwani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIF5HRqafp6-a3HMG54CxPkG_y5D_iDJvM287H9w=s64","userId":"06734995681986157428"}},"outputId":"c73a28e9-126b-4881-84e2-107e7e5e780d","colab":{"base_uri":"https://localhost:8080/"}},"source":["id2tag"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'I-product',\n"," 1: 'I-location',\n"," 2: 'B-corporation',\n"," 3: 'B-group',\n"," 4: 'I-creative-work',\n"," 5: 'B-person',\n"," 6: 'B-creative-work',\n"," 7: 'I-corporation',\n"," 8: 'I-group',\n"," 9: 'I-person',\n"," 10: 'O',\n"," 11: 'B-product',\n"," 12: 'B-location'}"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"Rg9UIR0xbDoI"},"source":["from transformers import DistilBertTokenizerFast\n","tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n","train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n","val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dYP1V3vJbr4S"},"source":["Now we arrive at a common obstacle with using pre-trained models for token-level classification: many of the tokens in the W-NUT corpus are not in DistilBert’s vocabulary. Bert and many models like it use a method called WordPiece Tokenization, meaning that single words are split into multiple tokens such that each token is likely to be in the vocabulary. For example, DistilBert’s tokenizer would split the Twitter handle @huggingface into the tokens ['@', 'hugging', '##face']. This is a problem for us because we have exactly one tag per token. If the tokenizer splits a token into multiple sub-tokens, then we will end up with a mismatch between our tokens and our labels.\n","\n","One way to handle this is to only train on the tag labels for the first subtoken of a split token. We can do this in 🤗 Transformers by setting the labels we wish to ignore to -100. In the example above, if the label for @HuggingFace is 3 (indexing B-corporation), we would set the labels of ['@', 'hugging', '##face'] to [3, -100, -100].\n","\n","Let’s write a function to do this. This is where we will use the offset_mapping from the tokenizer as mentioned above. For each sub-token returned by the tokenizer, the offset mapping gives us a tuple indicating the sub-token’s start position and end position relative to the original token it was split from. That means that if the first position in the tuple is anything other than 0, we will set its corresponding label to -100. While we’re at it, we can also set labels to -100 if the second position of the offset mapping is 0, since this means it must be a special token like [PAD] or [CLS]."]},{"cell_type":"code","metadata":{"id":"K3EWKzkKcuac"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gyhayDGpbGPF"},"source":["import numpy as np\n","\n","def encode_tags(tags, encodings):\n","    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n","    encoded_labels = []\n","    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n","        # create an empty array of -100\n","        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n","        arr_offset = np.array(doc_offset)\n","\n","        # set labels whose first offset position is 0 and the second is not 0\n","        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n","        encoded_labels.append(doc_enc_labels.tolist())\n","\n","    return encoded_labels\n","\n","train_labels = encode_tags(train_tags, train_encodings)\n","val_labels = encode_tags(val_tags, val_encodings)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bx0vUBtQcwBF"},"source":["import torch\n","\n","class WNUTDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n","val_encodings.pop(\"offset_mapping\")\n","train_dataset = WNUTDataset(train_encodings, train_labels)\n","val_dataset = WNUTDataset(val_encodings, val_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LDAttzQzc2TG"},"source":["\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4OpOcMcSdAlZ","executionInfo":{"status":"ok","timestamp":1605498533931,"user_tz":300,"elapsed":49223,"user":{"displayName":"pooja bhojwani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIF5HRqafp6-a3HMG54CxPkG_y5D_iDJvM287H9w=s64","userId":"06734995681986157428"}},"outputId":"ecf36f77-29e4-4046-9aaa-5c903e198089","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from transformers import DistilBertForTokenClassification, Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',          # output directory\n","    num_train_epochs=3,              # total number of training epochs\n","    per_device_train_batch_size=16,  # batch size per device during training\n","    per_device_eval_batch_size=16,   # batch size for evaluation\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    logging_dir='./logs',            # directory for storing logs\n","    logging_steps=10,\n",")\n","model = DistilBertForTokenClassification.from_pretrained('distilbert-base-cased', num_labels=len(unique_tags)) # add the layers to classify the tokens. len of layer = number of tagsb\n","\n","trainer = Trainer(\n","    model=model,                         # the instantiated 🤗 Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=val_dataset             # evaluation dataset\n",")\n","\n","trainer.train()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='510' max='510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [510/510 00:45, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>2.345685</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>2.253268</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>2.082476</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.786913</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.417712</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.834326</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.445556</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.321534</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.303329</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.223299</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.243941</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.227287</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.256337</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.180393</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.221783</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.229390</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.181898</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.187941</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.192780</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.165088</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.167802</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>0.126199</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>0.165955</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.118195</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.165488</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>0.193845</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>0.157152</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.168304</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>0.118518</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.162105</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>0.118202</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>0.204021</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>0.111711</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>0.161133</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.071764</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>0.104813</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>0.125229</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>0.103589</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>0.120773</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.081770</td>\n","    </tr>\n","    <tr>\n","      <td>410</td>\n","      <td>0.075073</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>0.099181</td>\n","    </tr>\n","    <tr>\n","      <td>430</td>\n","      <td>0.091624</td>\n","    </tr>\n","    <tr>\n","      <td>440</td>\n","      <td>0.087904</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>0.075636</td>\n","    </tr>\n","    <tr>\n","      <td>460</td>\n","      <td>0.069025</td>\n","    </tr>\n","    <tr>\n","      <td>470</td>\n","      <td>0.084732</td>\n","    </tr>\n","    <tr>\n","      <td>480</td>\n","      <td>0.065161</td>\n","    </tr>\n","    <tr>\n","      <td>490</td>\n","      <td>0.082568</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.134584</td>\n","    </tr>\n","    <tr>\n","      <td>510</td>\n","      <td>0.083853</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=510, training_loss=0.3489577648686428)"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"RFUDQuS6gRe5","executionInfo":{"status":"ok","timestamp":1605498541316,"user_tz":300,"elapsed":2315,"user":{"displayName":"pooja bhojwani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIF5HRqafp6-a3HMG54CxPkG_y5D_iDJvM287H9w=s64","userId":"06734995681986157428"}},"outputId":"e09cb090-8247-4794-b9da-830a50a7a5e7","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["trainer.evaluate()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='43' max='43' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [43/43 00:01]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["{'epoch': 3.0, 'eval_loss': 0.1142309382557869}"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"id":"2vAabppKlQIi","executionInfo":{"status":"ok","timestamp":1605498547363,"user_tz":300,"elapsed":1519,"user":{"displayName":"pooja bhojwani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIF5HRqafp6-a3HMG54CxPkG_y5D_iDJvM287H9w=s64","userId":"06734995681986157428"}},"outputId":"e4e7d5ad-3c6d-4040-e60d-6f3963132b72","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OBkvhf-Sp71s"},"source":["Save the model to drive, uncomment"]},{"cell_type":"code","metadata":{"id":"V8zl0c4eer46"},"source":["\n","model_dir = '/content/gdrive/My Drive/Colab Notebooks/'\n","path = model_dir + 'wnut_ner_pt_distilbert_uncased/model'\n","trainer.save_model(path)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oVnT1WUnoPqr"},"source":["Load the model from Drive, load the model from your saved weights, and the same tokenizer used for training. Then add it to pipeline for easy use. Dont forget to mount"]},{"cell_type":"code","metadata":{"id":"3UdhqnU2n7PS"},"source":["from transformers import DistilBertForTokenClassification, DistilBertTokenizerFast, pipeline\n","model_dir = '/content/gdrive/My Drive/Colab Notebooks/'\n","path = model_dir + 'wnut_ner_pt_distilbert_uncased/model'\n","model = DistilBertForTokenClassification.from_pretrained(path)\n","tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n","nlp = pipeline('ner',model= model,tokenizer=tokenizer)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JVG13U6CKRRP","executionInfo":{"status":"ok","timestamp":1605498557908,"user_tz":300,"elapsed":295,"user":{"displayName":"pooja bhojwani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIF5HRqafp6-a3HMG54CxPkG_y5D_iDJvM287H9w=s64","userId":"06734995681986157428"}},"outputId":"c648439d-4357-4c68-869e-0b170c69185b","colab":{"base_uri":"https://localhost:8080/"}},"source":["id2tag"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'I-product',\n"," 1: 'I-location',\n"," 2: 'B-corporation',\n"," 3: 'B-group',\n"," 4: 'I-creative-work',\n"," 5: 'B-person',\n"," 6: 'B-creative-work',\n"," 7: 'I-corporation',\n"," 8: 'I-group',\n"," 9: 'I-person',\n"," 10: 'O',\n"," 11: 'B-product',\n"," 12: 'B-location'}"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"nwqLsLDOqrz0"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ol8BaAofpv8A","executionInfo":{"status":"ok","timestamp":1605498574223,"user_tz":300,"elapsed":1295,"user":{"displayName":"pooja bhojwani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIF5HRqafp6-a3HMG54CxPkG_y5D_iDJvM287H9w=s64","userId":"06734995681986157428"}},"outputId":"fa97b35f-f5f5-4c06-ce0d-184732a426b5","colab":{"base_uri":"https://localhost:8080/"}},"source":["text =\"There's a lot of people showing off their iPhones on facebook today , so &lt; so is at such a place , it 's really not that interesting ;o )\"\n","print(nlp(text))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[{'word': 'There', 'score': 0.9995560050010681, 'entity': 'LABEL_10', 'index': 1}, {'word': \"'\", 'score': 0.9996503591537476, 'entity': 'LABEL_10', 'index': 2}, {'word': 's', 'score': 0.9996324181556702, 'entity': 'LABEL_10', 'index': 3}, {'word': 'a', 'score': 0.99964839220047, 'entity': 'LABEL_10', 'index': 4}, {'word': 'lot', 'score': 0.999610960483551, 'entity': 'LABEL_10', 'index': 5}, {'word': 'of', 'score': 0.9996151328086853, 'entity': 'LABEL_10', 'index': 6}, {'word': 'people', 'score': 0.9996602535247803, 'entity': 'LABEL_10', 'index': 7}, {'word': 'showing', 'score': 0.9995064735412598, 'entity': 'LABEL_10', 'index': 8}, {'word': 'off', 'score': 0.9992566704750061, 'entity': 'LABEL_10', 'index': 9}, {'word': 'their', 'score': 0.9982964396476746, 'entity': 'LABEL_10', 'index': 10}, {'word': 'iPhone', 'score': 0.8602146506309509, 'entity': 'LABEL_11', 'index': 11}, {'word': '##s', 'score': 0.8072671890258789, 'entity': 'LABEL_10', 'index': 12}, {'word': 'on', 'score': 0.9979084134101868, 'entity': 'LABEL_10', 'index': 13}, {'word': 'face', 'score': 0.6067138314247131, 'entity': 'LABEL_2', 'index': 14}, {'word': '##book', 'score': 0.4707931578159332, 'entity': 'LABEL_10', 'index': 15}, {'word': 'today', 'score': 0.9996068477630615, 'entity': 'LABEL_10', 'index': 16}, {'word': ',', 'score': 0.9997011423110962, 'entity': 'LABEL_10', 'index': 17}, {'word': 'so', 'score': 0.9996880292892456, 'entity': 'LABEL_10', 'index': 18}, {'word': '&', 'score': 0.9993385672569275, 'entity': 'LABEL_10', 'index': 19}, {'word': 'l', 'score': 0.9991810917854309, 'entity': 'LABEL_10', 'index': 20}, {'word': '##t', 'score': 0.9994592666625977, 'entity': 'LABEL_10', 'index': 21}, {'word': ';', 'score': 0.9995564818382263, 'entity': 'LABEL_10', 'index': 22}, {'word': 'so', 'score': 0.9996957778930664, 'entity': 'LABEL_10', 'index': 23}, {'word': 'is', 'score': 0.9996917843818665, 'entity': 'LABEL_10', 'index': 24}, {'word': 'at', 'score': 0.9996660947799683, 'entity': 'LABEL_10', 'index': 25}, {'word': 'such', 'score': 0.9995676875114441, 'entity': 'LABEL_10', 'index': 26}, {'word': 'a', 'score': 0.9996892809867859, 'entity': 'LABEL_10', 'index': 27}, {'word': 'place', 'score': 0.9996312856674194, 'entity': 'LABEL_10', 'index': 28}, {'word': ',', 'score': 0.9996795654296875, 'entity': 'LABEL_10', 'index': 29}, {'word': 'it', 'score': 0.9997170567512512, 'entity': 'LABEL_10', 'index': 30}, {'word': \"'\", 'score': 0.9996822476387024, 'entity': 'LABEL_10', 'index': 31}, {'word': 's', 'score': 0.9996573328971863, 'entity': 'LABEL_10', 'index': 32}, {'word': 'really', 'score': 0.9996998310089111, 'entity': 'LABEL_10', 'index': 33}, {'word': 'not', 'score': 0.9996107220649719, 'entity': 'LABEL_10', 'index': 34}, {'word': 'that', 'score': 0.9995812773704529, 'entity': 'LABEL_10', 'index': 35}, {'word': 'interesting', 'score': 0.9996774792671204, 'entity': 'LABEL_10', 'index': 36}, {'word': ';', 'score': 0.9995659589767456, 'entity': 'LABEL_10', 'index': 37}, {'word': 'o', 'score': 0.9996172189712524, 'entity': 'LABEL_10', 'index': 38}, {'word': ')', 'score': 0.9990108013153076, 'entity': 'LABEL_10', 'index': 39}]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rATq7FMqqyjr","executionInfo":{"status":"ok","timestamp":1605498605560,"user_tz":300,"elapsed":1028,"user":{"displayName":"pooja bhojwani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIF5HRqafp6-a3HMG54CxPkG_y5D_iDJvM287H9w=s64","userId":"06734995681986157428"}},"outputId":"382370c6-affa-49a2-a225-99edb1145cd2","colab":{"base_uri":"https://localhost:8080/"}},"source":["for item in (nlp(text)):\n","  word = item['word']\n","  entity = id2tag[int(item['entity'].split(\"_\")[1])]\n","  print (word, entity)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There O\n","' O\n","s O\n","a O\n","lot O\n","of O\n","people O\n","showing O\n","off O\n","their O\n","iPhone B-product\n","##s O\n","on O\n","face B-corporation\n","##book O\n","today O\n",", O\n","so O\n","& O\n","l O\n","##t O\n","; O\n","so O\n","is O\n","at O\n","such O\n","a O\n","place O\n",", O\n","it O\n","' O\n","s O\n","really O\n","not O\n","that O\n","interesting O\n","; O\n","o O\n",") O\n"],"name":"stdout"}]}]}